{"id":"q1","question":"Summarize the main contribution of the most relevant paper in my corpus.","answer":"**The paper \"AI Meets Brain: A Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents\" (arXiv:2512.23343v1) provides a comprehensive interdisciplinary synthesis bridging cognitive neuroscience and LLM-driven autonomous agents.**\n\nIts key contributions include:\n- Defining memory's role across neuroscience, LLMs, and agents.\n- Comparative analysis of memory taxonomies (e.g., episodic/semantic, short/long-term; inside-trail/cross-trail), storage, and lifecycle (extraction, update, retrieval, forgetting) in biological vs. artificial systems.\n- Reviews of agent memory benchmarks, security (attacks/defenses), and future directions like multimodal memory and skill acquisition.","configHash":"8d2ae1ada3f1042bef769ee6ab8bc4919dfabe3fd72357ac6299164b2c18dea1","timingsMs":{"rewrite":18598.975541,"embed.query":150.20716700000048,"retrieve.docs":5.514709000002767,"rerank":5288.010917,"retrieve.memory":0.012875000000349246,"generate":4290.1108330000025},"usage":{"promptTokens":15503,"completionTokens":833,"totalTokens":16336},"dollars":0,"judgeUsage":{"promptTokens":3220,"completionTokens":765,"totalTokens":3985},"judge":{"correctness":5,"groundedness":5,"memoryUse":1,"clarity":5,"notes":"Accurately identifies the most prominent paper from sources and summarizes contributions directly supported by snippets (e.g., taxonomies in [S17], title/authors in [S29]). No prior preferences/context to leverage."},"weightedScore":4.2,"retrievedSources":[{"citation":"S30","documentPath":"../2512.23343v1.pdf","chunkId":45},{"citation":"S1","documentPath":"../2512.23343v1.pdf","chunkId":29},{"citation":"S17","documentPath":"../2512.23343v1.pdf","chunkId":9},{"citation":"S18","documentPath":"../2512.23343v1.pdf","chunkId":41},{"citation":"S29","documentPath":"../2512.23343v1.pdf","chunkId":1}],"rewrittenQuery":"Main contribution of the most relevant paper in your corpus"}
{"id":"q2","question":"What benchmarks are mentioned for evaluating agent memory? Cite sources.","answer":"**Semantic-oriented benchmarks** evaluate internal state maintenance and cognitive abilities like fidelity (Fid.), dynamics (Dyn.), and generalization (Gen.).[S29]\n\nFrom Table 1 [S29]:\n- LifelongAgentBench [389] (lifelong learning, text, 1,400)\n- PrefEval [385] (personalized preference following, text, 3,000)\n- MPR [380] (multi-hop personalized reasoning, text, 108,000)\n- StreamBench [303] (continuous online learning, text, 9,702)\n- Madial-Bench [102] (memory-augmented dialogue, text, 160)\n- HotpotQA [336] (multi-hop QA, text, 112,779)\n- PerLTQA [68] (personalized memory, text, 8,593)\n- StoryBench [271] (interactive fiction, text, -)\n- SHARE [146] (skill learning/transfer, text, 1,201)\n- MovieChat-1K [246] (long video QA, text+movie, 15,000)\n- Mobile-Agent-E [293] (complex mobile tasks, text+image, 25)\n- M3-Agent [190] (long-form video QA/memory, text+audio+video, 5,510)\n- Text2Mem Bench [276] (memory operations, text, -)\n- SoMoSiMu-Bench [204] (social simulation, text+structured, 6,700)\n- MemDaily [377] (personalized reasoning, text, 26,003)\n\nAdditional mentions: LTMBenchmark [36], StoryBench [272], MemoryAgentBench [110], Evo-Memory [296], MemBench [5], HaluMem [39].[S7]\n\n**Episodic-oriented benchmarks** assess performance gains in downstream tasks.[S29]\n\nFrom Table 2 [S29]:\n- WebChoreArena [199] (memory+calculation, text+image, 532)\n- ScienceWorld [283] (scientific experiments, text, 7,200)\n- WebArena [397] (web operations, text+image, 812)\n- WebShop [337] (e-commerce, text+image, 12,087)\n- ToolBench [227] (tool/API use, text, 142,950)\n- GAIA [197] (general AI assistants, text+image+spreadsheet, 466)\n- BabyAI [52] (language learning, text+symbolic, 100,000+)\n- xBench-DS [43] (business tasks, text, 100)\n- AgentOccam [330] (web interaction, text+image, 812)\n- StuLife [33] (shared memory tasks, text, 1,284)\n- Mind2Web [62] (web agent tasks, text+image+file, 2,350)\n- PersonalWAB [31] (personalized web tasks, text+web API, 9,000)\n\nOther mentions: DialSim [377], LongMemEval [304].[S16][S29][S30]","configHash":"8d2ae1ada3f1042bef769ee6ab8bc4919dfabe3fd72357ac6299164b2c18dea1","timingsMs":{"rewrite":1777.9704579999961,"embed.query":34.40337500000169,"retrieve.docs":3.508332999997947,"rerank":6194.5979579999985,"retrieve.memory":0.0021249999990686774,"generate":8778.249000000003},"usage":{"promptTokens":17265,"completionTokens":1455,"totalTokens":18720},"dollars":0,"judgeUsage":{"promptTokens":3880,"completionTokens":806,"totalTokens":4686},"judge":{"correctness":5,"groundedness":5,"memoryUse":1,"clarity":5},"weightedScore":4.2,"retrievedSources":[{"citation":"S16","documentPath":"../2512.23343v1.pdf","chunkId":46},{"citation":"S29","documentPath":"../2512.23343v1.pdf","chunkId":23},{"citation":"S30","documentPath":"../2512.23343v1.pdf","chunkId":34},{"citation":"S7","documentPath":"../2512.23343v1.pdf","chunkId":27},{"citation":"S1","documentPath":"../memory/README.md","chunkId":62},{"citation":"S3","documentPath":"../mem-rag/README.md","chunkId":59},{"citation":"S14","documentPath":"../2512.23343v1.pdf","chunkId":2},{"citation":"S2","documentPath":"../2512.23343v1.pdf","chunkId":6},{"citation":"S13","documentPath":"../mem-rag/docs/EXPERIMENT.md","chunkId":60}],"rewrittenQuery":"agent memory evaluation benchmarks MMLU, BIG-bench, HotpotQA, TriviaQA, WebQuestions, SQuAD"}
